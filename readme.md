=====

2025.02.15
ローカル環境でLLMとチャットするプログラム

＜ 構成 ＞
test_LlamaCpp.py：本体
release_memory.py：メモリ解放ライブラリ
readme.md：このファイル
LLMモデルはユーザー自身で用意してください!!

Python3.13.0環境で作成
Gemini Code Assistで補完
streamlitで動作確認済み
ローカル環境のCPUのみで駆動
※ GPUを使う場合は、LLMのモデルをGPU用に変更する必要があります。
※ トークン数は最適解が見つからないため、適宜要調整でお願いします🙇‍♂️🙇‍♀️🙇
※ メモリ解放ソフトとを併用すると、より効果的に使いやすくなります。


＜ 参照コード ＞
https://goodsystem.jp/ai/local-llm-easy-setup-ai-chatbot-how-to.html

元のコードに対して追加した部分
2025.03.21
・アプリ上で回答後の会話履歴とリセット機能を追加
・release_memory.pyにメモリ解放を行う機能を追加(外部ライブラリ化)
  ※同ファイルは単体でも使用可能
・性能計測として、次の3項目を確認できるよう変更
  1：レスポンス時間
  2：メモリー使用量
  3：CPU使用率

2025.04.01
・メモリ解放コードが二重で実行される問題を修正


細かい部分は各自で調整してみてください😊

=====